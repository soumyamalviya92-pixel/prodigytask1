{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "19wQKW7X3eB9EvW-_bxDeRmtt2GNkHCZc",
      "authorship_tag": "ABX9TyNza8c8UNsz0t51EvCpA/57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyamalviya92-pixel/prodigytask1/blob/main/prodigytask1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets\n"
      ],
      "metadata": {
        "id": "LHJObIXvG5N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "id": "mFsF-9xbHBjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Artificial Intelligence is\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_length=50\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "WD0QWK0cHUrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data.txt\", \"w\") as f:\n",
        "    f.write(\"AI is the future of technology.\\n\")\n",
        "    f.write(\"Machine learning helps computers learn.\\n\")\n",
        "    f.write(\"Generative AI can create human-like text.\\n\")"
      ],
      "metadata": {
        "id": "ebzuN-eJHhvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "OgMSmKvDKCnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data.txt\", \"w\") as f:\n",
        "    f.write(\"AI is the future of technology.\\n\")\n",
        "    f.write(\"Machine learning helps computers learn.\\n\")\n",
        "    f.write(\"Generative AI can create human-like text.\\n\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"data.txt\"})\n"
      ],
      "metadata": {
        "id": "osw-5udGKWwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5JlSYkmqcIn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "uU4rNcgxLToe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    encoding = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64\n",
        "    )\n",
        "    encoding[\"labels\"] = encoding[\"input_ids\"]\n",
        "    return encoding\n",
        "\n",
        "tokenized_data = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Z3HS_qhhLf58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "862277ef"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,              # reduce epochs\n",
        "    per_device_train_batch_size=1,    # smaller batch\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    fp16=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8353334b"
      },
      "source": [
        "def tokenize_function(examples):\n",
        "    encoding = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64\n",
        "    )\n",
        "    encoding[\"labels\"] = encoding[\"input_ids\"]\n",
        "    return encoding\n",
        "\n",
        "tokenized_data = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,              # reduce epochs\n",
        "    per_device_train_batch_size=1,    # smaller batch\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    fp16=True\n",
        ")"
      ],
      "metadata": {
        "id": "zQtvrfwVQkOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Generative AI can\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_length=60\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "rt3gWKlsQueg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"gpt2-finetuned\")\n"
      ],
      "metadata": {
        "id": "0vKfwFMhQzLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "MI-KQzHCdU66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "sT9pY3wJeEJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5e82e97"
      },
      "source": [
        "import os\n",
        "print(os.listdir('sample_data'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}